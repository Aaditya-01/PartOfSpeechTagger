{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 1730,
     "status": "ok",
     "timestamp": 1581967511483,
     "user": {
      "displayName": "Tiago Duque",
      "photoUrl": "https://lh4.googleusercontent.com/-psXNkC6AmrI/AAAAAAAAAAI/AAAAAAAAAeo/oxVwy32BXiE/s64/photo.jpg",
      "userId": "05096291092060669756"
     },
     "user_tz": 180
    },
    "id": "4B5gdrh7Z_Ic",
    "outputId": "d0e59222-b640-4ed5-b8ae-329a2868c41c"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('treebank')\n",
    "from nltk.corpus import treebank\n",
    "penn_treebank = []\n",
    "for fileid in treebank.fileids():\n",
    "  tokens = []\n",
    "  tags = []\n",
    "  for word, tag in treebank.tagged_words(fileid):\n",
    "    tokens.append(word)\n",
    "    tags.append(tag)\n",
    "  penn_treebank.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: conllu in e:\\anaconda\\lib\\site-packages (4.4.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install conllu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from io import open\n",
    "from conllu import parse_incr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "executionInfo": {
     "elapsed": 32880,
     "status": "ok",
     "timestamp": 1581967542641,
     "user": {
      "displayName": "Tiago Duque",
      "photoUrl": "https://lh4.googleusercontent.com/-psXNkC6AmrI/AAAAAAAAAAI/AAAAAAAAAeo/oxVwy32BXiE/s64/photo.jpg",
      "userId": "05096291092060669756"
     },
     "user_tz": 180
    },
    "id": "4WwZYkNr1bPN",
    "outputId": "fc755c50-ed5e-44b2-9dda-875a5fe1bbaa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['P', 'o', 'l', 'i', 'c', 'i', 'e', 's', ' ', 'o', 'f', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'i', 's', 'a', 't', 'i', 'o', 'n', ' ', 's', 'h', 'o', 'u', 'l', 'd', ' ', 'b', 'e', ' ', 'c', 'o', 'n', 's', 'i', 'd', 'e', 'r', 'e', 'd', ' ', 'a', 's', ' ', 'r', 'e', 's', 'p', 'o', 'n', 's', 'e', 's', ' ', 't', 'o', ' ', 's', 'e', 'v', 'e', 'r', 'a', 'l', ' ', 'd', 'i', 's', 't', 'i', 'n', 'c', 't', ' ', 'p', 'r', 'e', 's', 's', 'u', 'r', 'e', 's', '.', ' ', 'F', 'i', 'r', 's', 't', ',', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'i', 's', 'a', 't', 'i', 'o', 'n', ' ', 'i', 's', ' ', 'a', ' ', 'r', 'e', 's', 'p', 'o', 'n', 's', 'e', ' ', 'b', 'y', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', ' ', 't', 'o', ' ', 'i', 'n', 't', 'e', 'r', 'n', 'a', 'l', ' ', 'f', 'o', 'r', 'c', 'e', 's', ' ', 's', 'u', 'c', 'h', ' ', 'a', 's', ' ', 'i', 'n', 'c', 'r', 'e', 'a', 's', 'i', 'n', 'g', ' ', 'f', 'i', 's', 'c', 'a', 'l', ' ', 'p', 'r', 'o', 'b', 'l', 'e', 'm', 's', ' ', '(', 'O', '’', 'C', 'o', 'n', 'n', 'o', 'r', ',', ' ', '1', '9', '7', '3', ')', '.', ' ', 'I', 't', ' ', 'p', 'r', 'o', 'v', 'i', 'd', 'e', 's', ' ', 'a', ' ', 'm', 'e', 'a', 'n', 's', ' ', 'o', 'f', ' ', 'l', 'e', 's', 's', 'e', 'n', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', '’', 's', ' ', 'f', 'i', 's', 'c', 'a', 'l', ' ', 'r', 'e', 's', 'p', 'o', 'n', 's', 'i', 'b', 'i', 'l', 'i', 't', 'i', 'e', 's', ' ', 'b', 'y', ' ', 'e', 'n', 'c', 'o', 'u', 'r', 'a', 'g', 'i', 'n', 'g', ' ', 't', 'h', 'e', ' ', 'd', 'e', 'v', 'e', 'l', 'o', 'p', 'm', 'e', 'n', 't', ' ', 'o', 'f', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'e', ' ', 'a', 'l', 't', 'e', 'r', 'n', 'a', 't', 'i', 'v', 'e', 's', ' ', 'w', 'h', 'i', 'c', 'h', ',', ' ', 't', 'h', 'e', 'o', 'r', 'e', 't', 'i', 'c', 'a', 'l', 'l', 'y', ' ', 'a', 't', ' ', 'l', 'e', 'a', 's', 't', ',', ' ', 'd', 'o', ' ', 'n', 'o', 't', ' ', 'd', 'r', 'a', 'w', ' ', 'u', 'p', 'o', 'n', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', '’', 's', ' ', 'f', 'i', 'n', 'a', 'n', 'c', 'i', 'a', 'l', ' ', 'r', 'e', 's', 'e', 'r', 'v', 'e', 's', '.', ' ', 'S', 'e', 'c', 'o', 'n', 'd', ',', ' ', 't', 'h', 'e', ' ', 'p', 'r', 'o', 'm', 'o', 't', 'i', 'o', 'n', ' ', 'o', 'f', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'e', ' ', 's', 'e', 'c', 't', 'o', 'r', ' ', 'a', 'c', 't', 'i', 'v', 'i', 't', 'y', ' ', 'i', 's', ' ', 'a', ' ', 'r', 'e', 's', 'p', 'o', 'n', 's', 'e', ' ', 't', 'o', ' ', 'p', 'r', 'e', 's', 's', 'u', 'r', 'e', 's', ' ', 'o', 'r', 'i', 'g', 'i', 'n', 'a', 't', 'i', 'n', 'g', ' ', '‘', 'o', 'u', 't', 's', 'i', 'd', 'e', '’', ' ', 't', 'h', 'e', ' ', 's', 't', 'a', 't', 'e', ' ', 'a', 'p', 'p', 'a', 'r', 'a', 't', 'u', 's', '.', ' ', 'T', 'h', 'e', 's', 'e', ' ', 'i', 'n', 'c', 'l', 'u', 'd', 'e', ' ', 'd', 'e', 'm', 'a', 'n', 'd', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'p', 'e', 'o', 'p', 'l', 'e', ' ', 'w', 'h', 'o', ' ', 's', 'e', 'e', ' ', 'a', ' ', 'l', 'a', 'r', 'g', 'e', ' ', 's', 't', 'a', 't', 'e', ' ', 'b', 'u', 'r', 'e', 'a', 'u', 'c', 'r', 'a', 'c', 'y', ' ', 'a', 's', ' ', 'i', 'n', 'e', 'f', 'f', 'i', 'c', 'i', 'e', 'n', 't', ' ', 'a', 'n', 'd', ' ', 'w', 'a', 's', 't', 'e', 'f', 'u', 'l', ',', ' ', 'd', 'e', 'm', 'a', 'n', 'd', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'b', 'u', 's', 'i', 'n', 'e', 's', 's', ' ', 'i', 'n', 't', 'e', 'r', 'e', 's', 't', 's', ' ', 'w', 'h', 'o', ' ', 'c', 'l', 'a', 'i', 'm', ' ', 't', 'h', 'a', 't', ' ', 't', 'h', 'e', 'y', ' ', 'c', 'a', 'n', ' ', 'o', 'v', 'e', 'r', 'c', 'o', 'm', 'e', ' ', 't', 'h', 'e', 's', 'e', ' ', 'i', 'n', 'e', 'f', 'f', 'i', 'c', 'i', 'e', 'n', 'c', 'i', 'e', 's', ',', ' ', 'a', 'n', 'd', ' ', 'p', 'r', 'e', 's', 's', 'u', 'r', 'e', 's', ' ', 'f', 'r', 'o', 'm', ' ', 'c', 'l', 'i', 'e', 'n', 't', ' ', 'g', 'r', 'o', 'u', 'p', 's', ' ', 'w', 'h', 'o', ' ', 's', 'e', 'e', 'k', ' ', 't', 'o', ' ', 'r', 'e', 'd', 'u', 'c', 'e', ' ', 't', 'h', 'e', 'i', 'r', ' ', 'd', 'e', 'p', 'e', 'n', 'd', 'e', 'n', 'c', 'y', ' ', 'o', 'n', ' ', 't', 'h', 'e', ' ', 'w', 'e', 'l', 'f', 'a', 'r', 'e', ' ', 's', 't', 'a', 't', 'e', ' ', 'b', 'y', ' ', 'h', 'a', 'v', 'i', 'n', 'g', ' ', 'm', 'o', 'r', 'e', ' ', 'c', 'o', 'n', 't', 'r', 'o', 'l', ' ', 'o', 'v', 'e', 'r', ' ', 't', 'h', 'e', ' ', 's', 'e', 'r', 'v', 'i', 'c', 'e', 's', ' ', 'o', 'n', ' ', 'w', 'h', 'i', 'c', 'h', ' ', 't', 'h', 'e', 'y', ' ', 'd', 'e', 'p', 'e', 'n', 'd', '.', ' ', 'C', 'l', 'e', 'a', 'r', 'l', 'y', ',', ' ', 't', 'h', 'i', 's', ' ', 'v', 'a', 'r', 'i', 'e', 't', 'y', ' ', 'o', 'f', ' ', 'c', 'a', 'l', 'l', 's', ' ', 'f', 'o', 'r', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'i', 's', 'a', 't', 'i', 'o', 'n', ' ', 'm', 'e', 'a', 'n', 's', ' ', 't', 'h', 'a', 't', ' ', 'i', 't', ' ', 'i', 's', ' ', 'n', 'o', 't', ' ', 'a', ' ', 'p', 'r', 'o', 'c', 'e', 's', 's', ' ', 'w', 'i', 't', 'h', ' ', 'a', ' ', 'u', 'n', 'i', 'f', 'o', 'r', 'm', ' ', 'o', 'u', 't', 'c', 'o', 'm', 'e', ';', ' ', 't', 'h', 'e', 'r', 'e', ' ', 'e', 'x', 'i', 's', 't', 's', ' ', 'a', ' ', 'c', 'o', 'r', 'r', 'e', 's', 'p', 'o', 'n', 'd', 'i', 'n', 'g', 'l', 'y', ' ', 'w', 'i', 'd', 'e', ' ', 'v', 'a', 'r', 'i', 'e', 't', 'y', ' ', 'o', 'f', ' ', 'f', 'o', 'r', 'm', 's', ' ', 'o', 'f', ' ', 'p', 'r', 'i', 'v', 'a', 't', 'i', 's', 'a', 't', 'i', 'o', 'n', '.']\n"
     ]
    }
   ],
   "source": [
    "#The imports needed to open and parse (interpret) the conllu file. At the end we'll have a list of dicts.\n",
    "\n",
    "data_file =\"Policies of privatisation should be considered as responses to several distinct pressures. First, privatisation is a response by the state to internal forces such as increasing fiscal problems (O’Connor, 1973). It provides a means of lessening the state’s fiscal responsibilities by encouraging the development of private alternatives which, theoretically at least, do not draw upon the state’s financial reserves. Second, the promotion of private sector activity is a response to pressures originating ‘outside’ the state apparatus. These include demands from people who see a large state bureaucracy as inefficient and wasteful, demands from business interests who claim that they can overcome these inefficiencies, and pressures from client groups who seek to reduce their dependency on the welfare state by having more control over the services on which they depend. Clearly, this variety of calls for privatisation means that it is not a process with a uniform outcome; there exists a correspondingly wide variety of forms of privatisation.\"\n",
    "ud_files = []\n",
    "for tokenlist in data_file:\n",
    "    ud_files.append(tokenlist)\n",
    "print(ud_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we iterate over all samples from the corpus and retrieve the word and the pre-labeled PoS tag (upostag). This will \n",
    "#be added as a list of tuples with a list of words and a list of their respective PoS tags (in the same order).\n",
    "ud_treebank = []\n",
    "for sentence in ud_files:\n",
    "    tokens = []\n",
    "    tags = []\n",
    "    for token in sentence:\n",
    "        tokens.append(token['form'])\n",
    "        tags.append(token['upostag'])\n",
    "    ud_treebank.append((tokens, tags))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WzzGnG10Ulv2"
   },
   "source": [
    "**Word of Caution!**\n",
    "\n",
    "Penn Treebank and UD Treebanks use *distinct tagsets*. \n",
    "\n",
    "We won't be able to interchange them unless we make a converter - also, we'll only be able to do so from Penn->UD, because Penn Treebank has tags more detailed than UD, and we won't be able to retrieve these details from the tags without a third function and a lot of effort.\n",
    "\n",
    "We'll only do that later, in our code.\n",
    "\n",
    "Let us continue with the explanation of the Tagger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bfD5ujGijuUF"
   },
   "source": [
    "#Extracting Features form Words\n",
    "\n",
    "Next, we have to create a function that is able to extract features from our words. These features will be used to predict the PoS.\n",
    "\n",
    "For that,  for each word, we'll pass the sentence and word index, and we'll provide a dict with the features.\n",
    "\n",
    "To explain about the feature set (can be changed, if you want), it is composed by:\n",
    "* Word: the word itself. Some words are always one PoS, others not.\n",
    "* is_first, is_last: check if it is the first or last in the sentence.\n",
    "* is_capitalized: first letter is caps? Maybe it is a proper noun...\n",
    "* is_all_caps or is_all_lower: checks for acronyms (or common words).\n",
    "* prefixes/suffixes: check word initialization/termination\n",
    "* prev_word/next_word: checks the preceding and succeding word.\n",
    "* has-hyphen: words with '-' may be adjectives.\n",
    "* is_numeric: for numbers.\n",
    "* capitals_inside: weird cases. Maybe nouns.\n",
    "\n",
    "The basis of this feature extraction method comes from two nice articles:\n",
    "* https://nlpforhackers.io/training-pos-tagger/\n",
    "* https://medium.com/analytics-vidhya/pos-tagging-using-conditional-random-fields-92077e5eaa31\n",
    "\n",
    "If you're wondering, yes, this encoding WILL need a lot of memory for training (if you're not using categorical variables).\n",
    "\n",
    "And we'll have to replicate this in our main code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-IyIaTSwoo-V"
   },
   "outputs": [],
   "source": [
    "#Regex module for checking alphanumeric values.\n",
    "import re\n",
    "def extract_features(sentence, index):\n",
    "  return {\n",
    "      'word':sentence[index],\n",
    "      'is_first':index==0,\n",
    "      'is_last':index ==len(sentence)-1,\n",
    "      'is_capitalized':sentence[index][0].upper() == sentence[index][0],\n",
    "      'is_all_caps': sentence[index].upper() == sentence[index],\n",
    "      'is_all_lower': sentence[index].lower() == sentence[index],\n",
    "      'is_alphanumeric': int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',sentence[index])))),\n",
    "      'prefix-1':sentence[index][0],\n",
    "      'prefix-2':sentence[index][:2],\n",
    "      'prefix-3':sentence[index][:3],\n",
    "      'prefix-3':sentence[index][:4],\n",
    "      'suffix-1':sentence[index][-1],\n",
    "      'suffix-2':sentence[index][-2:],\n",
    "      'suffix-3':sentence[index][-3:],\n",
    "      'suffix-3':sentence[index][-4:],\n",
    "      'prev_word':'' if index == 0 else sentence[index-1],\n",
    "      'next_word':'' if index < len(sentence) else sentence[index+1],\n",
    "      'has_hyphen': '-' in sentence[index],\n",
    "      'is_numeric': sentence[index].isdigit(),\n",
    "      'capitals_inside': sentence[index][1:].lower() != sentence[index][1:]\n",
    "  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CYS5r1_m6Yr9"
   },
   "source": [
    "We now prepare the dataset for use in Machine Learning algorithms.\n",
    "\n",
    "There are two steps (three, if we're doing deep learning, but that's for later) to it: \n",
    "* Defining a function to transform the corpus to a more datsetish format.\n",
    "* Then, divide the encoded data into training and testing sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1hiniE_wzPOC"
   },
   "outputs": [],
   "source": [
    "#Ater defining the extract_features, we define a simple function to transform our data in a more 'datasetish' format.\n",
    "#This function returns the data as two lists, one of Dicts of features and the other with the labels.\n",
    "def transform_to_dataset(tagged_sentences):\n",
    "  X, y = [], []\n",
    "  for sentence, tags in tagged_sentences:\n",
    "    sent_word_features, sent_tags = [],[]\n",
    "    for index in range(len(sentence)):\n",
    "        sent_word_features.append(extract_features(sentence, index)),\n",
    "        sent_tags.append(tags[index])\n",
    "    X.append(sent_word_features)\n",
    "    y.append(sent_tags)\n",
    "  return X, y\n",
    "\n",
    "#We divide the set BEFORE encoding. Why? To have full sentences in training/testing sets. When we encode, we do not encode\n",
    "#a sentence, but its words instead.\n",
    "\n",
    "#First, for the Penn treebank.\n",
    "penn_train_size = int(0.8*len(penn_treebank))\n",
    "penn_training = penn_treebank[:penn_train_size]\n",
    "penn_testing = penn_treebank[penn_train_size:]\n",
    "X_penn_train, y_penn_train = transform_to_dataset(penn_training)\n",
    "X_penn_test, y_penn_test = transform_to_dataset(penn_testing)\n",
    "\n",
    "#Then, for UD Treebank.\n",
    "ud_train_size = int(0.8*len(ud_treebank))\n",
    "ud_training = ud_treebank[:ud_train_size]\n",
    "ud_testing = ud_treebank[ud_train_size:]\n",
    "X_ud_train, y_ud_train = transform_to_dataset(ud_training)\n",
    "X_ud_test, y_ud_test = transform_to_dataset(ud_testing)\n",
    "\n",
    "#Third step, vectorize datasets. For that we use sklearn DictVectorizer\n",
    "#WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IFc51jNtDptW"
   },
   "source": [
    "# Training a Tagger\n",
    "\n",
    "Now, we can train supervised machine learning algorithms to PoS Tagging.\n",
    "\n",
    "We'll use the Conditional Random Fields (CRF) algorithm. Here's a brief explanation:\n",
    "\n",
    "* **CRF**: A variation of Markov Random Field. Okay, that might not have helped. It is a discriminative model that, in a quick summary, evaluates the probabilities that a set of states are dependant or not between themselves based on a set of observations. In this case, it evaluates the probabilities that a word observed in a context (defined by the above mentioned features) belongs to a specific PoS. In training time, it takes what is the best state given the set of current observations and probabilities.\n",
    "\n",
    "<div>\n",
    "<img src=\"https://miro.medium.com/max/681/1*8hOWH7YF5INMF2OPhKjVxA.png\" width=\"400\"/>\n",
    "</div>\n",
    "\n",
    "Want more math? Read this: https://towardsdatascience.com/conditional-random-fields-explained-e5b8256da776\n",
    "\n",
    "So, to achieve this, we'll use scikit learn (sklearn) and a sklearn compatible crf suite (skleran_crfsuit). If you don't know what is sklearn, [read this](https://scikit-learn.org/stable/getting_started.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "executionInfo": {
     "elapsed": 119226,
     "status": "ok",
     "timestamp": 1581967629010,
     "user": {
      "displayName": "Tiago Duque",
      "photoUrl": "https://lh4.googleusercontent.com/-psXNkC6AmrI/AAAAAAAAAAI/AAAAAAAAAeo/oxVwy32BXiE/s64/photo.jpg",
      "userId": "05096291092060669756"
     },
     "user_tz": 180
    },
    "id": "OHTkotyWpd28",
    "outputId": "8c2c8bde-33f7-4299-f85e-6a3c6edd48f3"
   },
   "outputs": [],
   "source": [
    "#Ignoring some warnings for the sake of readability.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#First, install sklearn_crfsuite, as it is not preloaded into Colab. \n",
    "!pip install sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF\n",
    "\n",
    "#This loads the model. Specifics are: \n",
    "#algorithm: methodology used to check if results are improving. Default is lbfgs (gradient descent).\n",
    "#c1 and c2:  coefficients used for regularization.\n",
    "#max_iterations: max number of iterations (DUH!)\n",
    "#all_possible_transitions: since crf creates a \"network\", of probability transition states,\n",
    "#this option allows it to map even \"connections\" not present in the data.\n",
    "penn_crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "#The fit method is the default name used by Machine Learning algorithms to start training.\n",
    "print(\"Started training on Penn Treebank corpus!\")\n",
    "penn_crf.fit(X_penn_train, y_penn_train)\n",
    "print(\"Finished training on Penn Treebank corpus!\")\n",
    "\n",
    "#Same for UD\n",
    "ud_crf = CRF(\n",
    "    algorithm='lbfgs',\n",
    "    c1=0.01,\n",
    "    c2=0.1,\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "print(\"Started training on UD corpus!\")\n",
    "ud_crf.fit(X_ud_train, y_ud_train)\n",
    "print(\"Finished training on UD corpus!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQvig1nQBcbA"
   },
   "source": [
    "# Checking the Results\n",
    "\n",
    "For that, we'll use a score method named balanced f-score. This score takes into account *precision* and *recall*.\n",
    "\n",
    "* **precision**: Considering the universe of tagged words, how many were correctly tagged?\n",
    "* **recall**: Considering the universe of correct tags, how many words were really correctly tagged?\n",
    "\n",
    "The distinction is in the direction you look. Precision looks at all tagged words to find how many are ok; Recall looks at correct tags to find how many were able to be \"guessed\".\n",
    "\n",
    "F-score is then calculated using these two. I won't go into the maths of it.  If you want,\n",
    "* You can read the wikipedia article here: https://en.wikipedia.org/wiki/F1_score\n",
    "* Or watch a neat simple video here: https://www.youtube.com/watch?v=j-EB6RqqjGI&ab_channel=CodeEmporium\n",
    "\n",
    "Also, here's the wikipedia image to help you understand:\n",
    "<div>\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/350px-Precisionrecall.svg.png\"/>\n",
    "</div>\n",
    "\n",
    "We won't go into the computations either. Let the package do its thing (after all, we're interested in NLP now, not in statistics):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 122663,
     "status": "ok",
     "timestamp": 1581967632451,
     "user": {
      "displayName": "Tiago Duque",
      "photoUrl": "https://lh4.googleusercontent.com/-psXNkC6AmrI/AAAAAAAAAAI/AAAAAAAAAeo/oxVwy32BXiE/s64/photo.jpg",
      "userId": "05096291092060669756"
     },
     "user_tz": 180
    },
    "id": "uTlJwNkF_0zs",
    "outputId": "4073c2df-6679-45db-83a5-65e135ed55dd"
   },
   "outputs": [],
   "source": [
    "#We'll use the sklearn_crfsuit own metrics to compute f1 score.\n",
    "from sklearn_crfsuite import metrics\n",
    "from sklearn_crfsuite import scorers\n",
    "print(\"## Penn ##\")\n",
    "\n",
    "#First calculate a prediction from test data, then we print the metrics for f-1 using the .flat_f1_score method.\n",
    "y_penn_pred=penn_crf.predict(X_penn_test)\n",
    "print(\"F1 score on Test Data\")\n",
    "print(metrics.flat_f1_score(y_penn_test, y_penn_pred,average='weighted',labels=penn_crf.classes_))\n",
    "#For the sake of clarification, we do the same for train data.\n",
    "y_penn_pred_train=penn_crf.predict(X_penn_train)\n",
    "print(\"F1 score on Training Data \")\n",
    "print(metrics.flat_f1_score(y_penn_train, y_penn_pred_train,average='weighted',labels=penn_crf.classes_))\n",
    "\n",
    "# This presents class wise score. Helps see which classes (tags) are the ones with most problems.\n",
    "print(\"Class wise score:\")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_penn_test, y_penn_pred, labels=penn_crf.classes_, digits=3\n",
    "))\n",
    "\n",
    "#Same for UD\n",
    "print(\"## UD ##\")\n",
    "\n",
    "y_ud_pred=ud_crf.predict(X_ud_test)\n",
    "print(\"F1 score on Test Data \")\n",
    "print(metrics.flat_f1_score(y_ud_test, y_ud_pred,average='weighted',labels=ud_crf.classes_))\n",
    "y_ud_pred_train=ud_crf.predict(X_ud_train)\n",
    "print(\"F1 score on Training Data \")\n",
    "print(metrics.flat_f1_score(y_ud_train, y_ud_pred_train,average='weighted',labels=ud_crf.classes_))\n",
    "\n",
    "### Look at class wise score\n",
    "print(\"Class wise score:\")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_ud_test, y_ud_pred, labels=ud_crf.classes_, digits=3\n",
    "))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJUcgYacWTaa"
   },
   "source": [
    "Not too shabby!\n",
    "\n",
    "Remember that State of the Art results for Penn Treebank are at 97% f1.\n",
    "\n",
    "Now, notice how UD is worse (90%)? Probably because there aren't many tags, so less variation and less classes for probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "But, wouldn't it be better if we could see it actually working?\n",
    "\n",
    "That's what the following cell does. It also helps us understand what we'll have to implement in our main algorithm for it to work.\n",
    "\n",
    "Feel free to play with the input phrase.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 122659,
     "status": "ok",
     "timestamp": 1581967632453,
     "user": {
      "displayName": "Tiago Duque",
      "photoUrl": "https://lh4.googleusercontent.com/-psXNkC6AmrI/AAAAAAAAAAI/AAAAAAAAAeo/oxVwy32BXiE/s64/photo.jpg",
      "userId": "05096291092060669756"
     },
     "user_tz": 180
    },
    "id": "C4rcQq-ubbuT",
    "outputId": "eb7ac322-ec2a-431b-d5c8-f8a8cd10e37e"
   },
   "outputs": [],
   "source": [
    "#First, we pass the sentence and \"quickly tokenize it\" - we've already done it in our code, so I'll just mock here with a split:\n",
    "sent = \"The tagger produced good results\"\n",
    "features = [extract_features(sent.split(), idx) for idx in range(len(sent.split()))]\n",
    "\n",
    "#Then we tell the algorithm to make a prediction on a single input (sentence). I'll do once for Penn Treebank and once for UD.\n",
    "penn_results = penn_crf.predict_single(features)\n",
    "ud_results = ud_crf.predict_single(features)\n",
    "\n",
    "#These line magics are just there to make it a neaty print, making a (word, POS) style print;\n",
    "penn_tups = [(sent.split()[idx], penn_results[idx]) for idx in range(len(sent.split()))]\n",
    "ud_tups = [(sent.split()[idx], ud_results[idx]) for idx in range(len(sent.split()))]\n",
    "\n",
    "#The results come out here! Notice the difference in tags.\n",
    "print(penn_tups)\n",
    "print(ud_tups)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Mn831Zxbckt"
   },
   "source": [
    "# Saving the Weights\n",
    "\n",
    "We will want to load this to our NLPTools, right? So we have to save the weights. This means saving the classifier we trained to be able to classify our tokens.\n",
    "\n",
    "To do it, we use Pickle, which is a Python package to save a readable binary file extension called \"pickle\". We'll later open this in our tool.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ul2KlQu4c5-N"
   },
   "outputs": [],
   "source": [
    "#import the pickle module\n",
    "import pickle\n",
    "\n",
    "#Simply dump! Use 'wb' in open to write bytes.\n",
    "\n",
    "penn_filename = 'penn_treebank_crf_postagger.sav'\n",
    "pickle.dump(penn_crf, open(penn_filename, 'wb'))\n",
    "\n",
    "ud_filename = 'ud_crf_postagger.sav'\n",
    "pickle.dump(ud_crf, open(ud_filename,'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6O2F5HijfWc8"
   },
   "source": [
    "To open the file, we just have to import the module and read the file using:\n",
    "\n",
    "`model = pickle.load(open(filename, 'rb'))`\n",
    "\n",
    "Great, we now have pickle files that can be loaded in our tool. Just download them using the lefthand file explorer and we're good to go!\n",
    "See you back at the article!"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "Copy of NLP Series POS Tagging.ipynb",
   "provenance": [
    {
     "file_id": "1d7LO_0665DYw6DrVJXXautJAJzHHqYOm",
     "timestamp": 1639241708607
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
